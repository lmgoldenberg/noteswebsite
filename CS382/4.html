<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>CS Notes</title>
  </head>
  <body>
    <main>
        <h1 id="memory">Memory</h1>
<h2 id="4-1">4.1</h2>
<p>Look at figure 4.0. Over the years, CPU cycle time has gotten shorter meaning faster instructions, but memory access time (DRAM) has not. So the gap between CPU and memory is increasing.</p>
<p> assume a is int array with length n, the code adds all elements of the array together</p>
<pre><code class="lang-C"><span class="hljs-keyword">int</span> <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) <span class="hljs-keyword">sum</span> += a[i];
<span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
</code></pre>
<p><strong>spacial locality</strong>: data with nearby addresses used closely together, in the example code this would be the array&#39;s elements
<strong>temporal locality</strong>: recently referenced data likely to be used again, in the example code this would be sum</p>
<p>C is a <strong>row-major order language</strong> meaning when handling two dimensional arrays, it stores rows next to each other. <strong>column-major order languages</strong> such as FORTRAN store columns next to each other.</p>
<p>Given these two example C programs, which one has better locality?</p>
<pre><code class="lang-C"> <span class="hljs-keyword">int</span> sum_array_rows(<span class="hljs-keyword">int</span> a[M][N]) {
   <span class="hljs-keyword">int</span> i, j, <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
   <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; M; i ++)
     <span class="hljs-keyword">for</span> (j = <span class="hljs-number">0</span>; j &lt; N; j ++)
       <span class="hljs-keyword">sum</span> += a[i][j];
 <span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
 }

 <span class="hljs-keyword">int</span> sum_array_cols(<span class="hljs-keyword">int</span> a[M][N]) {
   <span class="hljs-keyword">int</span> i, j, <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
   <span class="hljs-keyword">for</span> (j = <span class="hljs-number">0</span>; j &lt; N; j ++)
     <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; M; i ++)
       <span class="hljs-keyword">sum</span> += a[i][j];
   <span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
</code></pre>
<p>Because C is a row-major order language, <code>sum_array_rows()</code>  has better locality in C because the inner loop accesses elements stored together. This is called <strong>stride-1 reference</strong></p>
<p><code>sum_array_cols()</code> needs to skip over N elements in each iteration since columns are not stored as closely together in C, meaning it has a <strong>stride-N reference.</strong> In column-major order languages, <code>sum_array_cols()</code> would have better locality. </p>
<p>Caching is the concept of if something is used frequently and together, to load them to a separate device that is faster than memory just once, and put them back in memory after.</p>
<p>Look at 4.1, disk seek is almost 10^9 times slower than cpu cycle time. Programs stored on the hard drive are sent to memory, making running programs much faster.</p>
<img src="./images3824/1.png" alt="">
<img src="./images3824/2.png" alt="">
<p>The faster a storage device is, the more expensive it is to store a byte. So faster devices end up storing less.</p>
<p>Registers are inside the CPU, so they are the fastest storage, but that means they are also the most expensive. Hence why there are only 32 in ARMv8.</p>
<p>If the table of devices(4.1 above) were to be organized in a diagram, it would be the pyramid in 4.4(above) called the <strong>memory hierarchy</strong>. The device at level k in the image is a cache of the device at level k+1. As k increases devices get larger, cheaper, but are also slower. The device at k stores data frequently used from k+1, so k-1 can access it faster. If data requested by k-1 is not at k, it will retrieve it from k+1.</p>
<h2 id="4-2">4.2</h2>
<p>In order to do computation, variables and programs must be loaded from memory. It takes 10^2 (100 clock cycles) to load data from main memory.</p>
<p><strong>Memory Transactions</strong></p>
<p>RAM is special because given an address we can jump right to that address.
Other storage devices require reading from the beginning.</p>
<p>4.5 shows the bus structure between a CPU chip and main memory. It has 3 main components besides CPU and main memory.
 <strong>System bus</strong> contains three major parts: control bus, address bus, and data bus;
 <strong>I/O Bridge</strong> connects I/O devices to the system bus to be controlled and used by CPU chip;
<strong>Memory Bus</strong> contains address, data, and control bus aswell, which has been covered earlier in Chapter 3. The role of control bus here is to indicate if this is a read or write transaction</p>
<p><strong>Read Transactions</strong></p>
<p>Primarily done by LDR instructions in ARMv8
Ex: <code>LDR X10, [X9]</code> will read the value at <code>X9</code> first, then put it on the system bus and memory bus through the bus interface and I/O bridge. Then it grabs the value stored at <code>X9</code> in memory, and transfers back the same path and stores the value into <code>X10</code>.</p>
<p><strong>Write Transactions</strong></p>
<p>Primarily done by STR instructions in ARMv8.
Ex: <code>STR X10, [X9]</code> will work very similar to a read transaction, but instead both <code>X9</code> and <code>X10</code> are read, and <code>X9</code> still goes on the memory bus, but <code>X10</code> will go on the data bus. Then it will be stored in main memory.</p>
<p><strong>Adding the Cache</strong></p>
<p>Memory transactions are too slow for the processor.
Cache is built of Static Ram/SRAM which is more expensive but faster. It is also smaller than main memory.</p>
<p>4.6 shows what 4.5 looks like once a cache is added on the CPU.</p>
<img src="./images3824/3.png" alt="">
<img src="./images3824/4.png" alt="">
<p>Here&#39;s an example of how a cache works, use figure 4.7 as a visualization</p>
<p>At the start the cache is empty. Then we run <code>LDR X0,[X1]</code>. Because the memory data at address X1(M[X1]) is not in the cache, we go to memory to retrieve it.</p>
<p>When we retrieve the data, we don&#39;t just bring the data but also a block of data which is the data at neighboring addresses in memory.</p>
<p>EX: if we load a double word that spans <code>0x1000</code> to <code>0x1008</code>, we would copy all data from <code>0x1000</code> to <code>0x1040</code> to the cache, which is a total of 8 double words. (remember addresses are in hex, 40 in hex = 64, 64/8 = 8)</p>
<p>Now when the CPU tries to read data in-between addresses 0x1000 and 0x1040, we can just copy it from the cache instead of going to main memory. When this happens, we say it&#39;s a <strong>hit</strong></p>
<p>If the CPU tries to read data that isn&#39;t in-between addresses 0x1000 and 0x1040, then we go back to main memory, and grab a new block of data. This new block of data overwrites the current block of data in the cache. This is called a <strong>miss</strong></p>
<p>This is why good special locality is important. Since the cache stores a chunk of memory, if the memory we are accessing together is close together, then the cache is much more likely to have a hit than a miss.</p>
<p><strong>Memory Organization</strong></p>
<p>In a cache, those blocks of data are called <strong>lines</strong>.
A cache has 3 parameters, S, E, and B. A cache will always have S=2^s <strong>sets</strong>, and each set will have E=2^e lines. </p>
<p>A line contains the following a<strong>valid bit</strong> referred to as v which indicates if the data stored in the line is valid or not. As well as B=2^b bytes that store the actual cached data copied from memory.</p>
<p>Total capacity of a cache = S*E*B bytes.
4.8(bottom next msg) shows an image of a cache.</p>
<p>Given an address, to check if the data is in the cache, the address split into three smaller parts. t bits for the tag, s bits for the set index, and b bits for the block offset.</p>
<p>First t bits are the tag. Next s bits are the set index. Last b bits for block offset.</p>
<img src="./images3824/5.png" alt="">
<p>From there we perform the following steps</p>
<ol>
<li>Use the set index in the address to locate a set.</li>
<li>Compare the tags in all lines of the set with the one in the address.
If a tag is a match, that is a hit, the data is in that line starting from the block offset.
If there isn&#39;t a match, it&#39;s a miss, so we go and copy the data from memory instead.</li>
</ol>
<p><strong>Direct-Mapped Cache</strong></p>
<ul>
<li>Simplest form of cache</li>
<li>Each set has only 1 line</li>
<li>Example on making a direct mapped cache below</li>
<li>Assumptions we can make for the example<ul>
<li>Main Memory uses 4 bit addresses, so total of 16 bytes of data, each byte is uniquely addressed.</li>
<li>Mini cache is direct mapped so 1 line per set</li>
<li>Cache has a total of 4 sets, so s=2</li>
<li>Each set can store 2 bytes of data so b=1</li>
<li>Each time we retrieve 1 byte of data
-The cache starts empty</li>
</ul></ul>
<p>Pictures are attached at bottom of message.</p>
<p>Because each address has 4 bits, and s=2 and b=1, t=4-2-1. So t=1</p>
<p>Now lets run the requests one at a time in this order: <code>0b0000</code>, <code>0b0001</code>, <code>0b0111</code>, <code>0b1000</code>, and <code>0b0000</code></p>
<p>Since we know the first t bits are for the tag, the next s bits are for set index, and the last b bits are for the block offset, and all bits are 0 for this address, all of those values will be 0.
Since the cache is empty, Set 00 has nothing, so it is empty, so it is a miss.
So we go to memory and grab <code>M[0b0000]</code> and <code>M[0b0001]</code>, we grab both because in the assumptions we know each set can store 2 bytes, so we grab 2 bytes of data.</p>
<p>After that we load 1 byte starting at offset 0 from that line, so <code>M[0b0000]</code> goes back to the register files.</p>
<p>Next we load <code>0b0001</code>, and it&#39;s 2 bits after the tag are 0b00, so we index to set 00. We notice the valid bit is turned on, and the tag also matches what is in the data. So we have a hit. Then we index by 1 since the last bit in this example is the block offset and we send the data which is <code>M[0001]</code> to the register file.</p>
<img src="./images3824/6.png" alt="">
<img src="./images3824/7.png" alt="">
<img src="./images3824/8.png" alt="">

</main>
  </body>
</html>