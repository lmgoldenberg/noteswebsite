<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>CS Notes</title>
  </head>
  <body>
    <main>
        <h1 id="memory">Memory</h1>
<h2 id="4-1">4.1</h2>
<p>Look at figure 4.0. Over the years, CPU cycle time has gotten shorter meaning faster instructions, but memory access time (DRAM) has not. So the gap between CPU and memory is increasing.</p>
<p> assume a is int array with length n, the code adds all elements of the array together</p>
<pre><code class="lang-C"><span class="hljs-keyword">int</span> <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) <span class="hljs-keyword">sum</span> += a[i];
<span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
</code></pre>
<p><strong>spacial locality</strong>: data with nearby addresses used closely together, in the example code this would be the array&#39;s elements
<strong>temporal locality</strong>: recently referenced data likely to be used again, in the example code this would be sum</p>
<p>C is a <strong>row-major order language</strong> meaning when handling two dimensional arrays, it stores rows next to each other. <strong>column-major order languages</strong> such as FORTRAN store columns next to each other.</p>
<p>Given these two example C programs, which one has better locality?</p>
<pre><code class="lang-C"> <span class="hljs-keyword">int</span> sum_array_rows(<span class="hljs-keyword">int</span> a[M][N]) {
   <span class="hljs-keyword">int</span> i, j, <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
   <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; M; i ++)
     <span class="hljs-keyword">for</span> (j = <span class="hljs-number">0</span>; j &lt; N; j ++)
       <span class="hljs-keyword">sum</span> += a[i][j];
 <span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
 }

 <span class="hljs-keyword">int</span> sum_array_cols(<span class="hljs-keyword">int</span> a[M][N]) {
   <span class="hljs-keyword">int</span> i, j, <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
   <span class="hljs-keyword">for</span> (j = <span class="hljs-number">0</span>; j &lt; N; j ++)
     <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; M; i ++)
       <span class="hljs-keyword">sum</span> += a[i][j];
   <span class="hljs-keyword">return</span> <span class="hljs-keyword">sum</span>;
</code></pre>
<p>Because C is a row-major order language, <code>sum_array_rows()</code>  has better locality in C because the inner loop accesses elements stored together. This is called <strong>stride-1 reference</strong></p>
<p><code>sum_array_cols()</code> needs to skip over N elements in each iteration since columns are not stored as closely together in C, meaning it has a <strong>stride-N reference.</strong> In column-major order languages, <code>sum_array_cols()</code> would have better locality. </p>
<p>Caching is the concept of if something is used frequently and together, to load them to a separate device that is faster than memory just once, and put them back in memory after.</p>
<p>Look at 4.1, disk seek is almost 10^9 times slower than cpu cycle time. Programs stored on the hard drive are sent to memory, making running programs much faster.</p>
<img src="./images3824/1.png" alt="img id 1, if you see this msg email me!">
<img src="./images3824/2.png" alt="img id 2, if you see this msg email me!">
<p>The faster a storage device is, the more expensive it is to store a byte. So faster devices end up storing less.</p>
<p>Registers are inside the CPU, so they are the fastest storage, but that means they are also the most expensive. Hence why there are only 32 in ARMv8.</p>
<p>If the table of devices(4.1 above) were to be organized in a diagram, it would be the pyramid in 4.4(above) called the <strong>memory hierarchy</strong>. The device at level k in the image is a cache of the device at level k+1. As k increases devices get larger, cheaper, but are also slower. The device at k stores data frequently used from k+1, so k-1 can access it faster. If data requested by k-1 is not at k, it will retrieve it from k+1.</p>
<h2 id="4-2">4.2</h2>
<p>In order to do computation, variables and programs must be loaded from memory. It takes 10^2 (100 clock cycles) to load data from main memory.</p>
<p><strong>Memory Transactions</strong></p>
<p>RAM is special because given an address we can jump right to that address.
Other storage devices require reading from the beginning.</p>
<p>4.5 shows the bus structure between a CPU chip and main memory. It has 3 main components besides CPU and main memory.
 <strong>System bus</strong> contains three major parts: control bus, address bus, and data bus;
 <strong>I/O Bridge</strong> connects I/O devices to the system bus to be controlled and used by CPU chip;
<strong>Memory Bus</strong> contains address, data, and control bus aswell, which has been covered earlier in Chapter 3. The role of control bus here is to indicate if this is a read or write transaction</p>
<p><strong>Read Transactions</strong></p>
<p>Primarily done by LDR instructions in ARMv8
Ex: <code>LDR X10, [X9]</code> will read the value at <code>X9</code> first, then put it on the system bus and memory bus through the bus interface and I/O bridge. Then it grabs the value stored at <code>X9</code> in memory, and transfers back the same path and stores the value into <code>X10</code>.</p>
<p><strong>Write Transactions</strong></p>
<p>Primarily done by STR instructions in ARMv8.
Ex: <code>STR X10, [X9]</code> will work very similar to a read transaction, but instead both <code>X9</code> and <code>X10</code> are read, and <code>X9</code> still goes on the memory bus, but <code>X10</code> will go on the data bus. Then it will be stored in main memory.</p>
<p><strong>Adding the Cache</strong></p>
<p>Memory transactions are too slow for the processor.
Cache is built of Static Ram/SRAM which is more expensive but faster. It is also smaller than main memory.</p>
<p>4.6 shows what 4.5 looks like once a cache is added on the CPU.</p>
<img src="./images3824/3.png" alt="img id 3, if you see this msg email me!">
<img src="./images3824/4.png" alt="img id 4, if you see this msg email me!">
<p>Here&#39;s an example of how a cache works, use figure 4.7 as a visualization</p>
<p>At the start the cache is empty. Then we run <code>LDR X0,[X1]</code>. Because the memory data at address X1(M[X1]) is not in the cache, we go to memory to retrieve it.</p>
<p>When we retrieve the data, we don&#39;t just bring the data but also a block of data which is the data at neighboring addresses in memory.</p>
<p>EX: if we load a double word that spans <code>0x1000</code> to <code>0x1008</code>, we would copy all data from <code>0x1000</code> to <code>0x1040</code> to the cache, which is a total of 8 double words. (remember addresses are in hex, 40 in hex = 64, 64/8 = 8)</p>
<p>Now when the CPU tries to read data in-between addresses 0x1000 and 0x1040, we can just copy it from the cache instead of going to main memory. When this happens, we say it&#39;s a <strong>hit</strong></p>
<p>If the CPU tries to read data that isn&#39;t in-between addresses 0x1000 and 0x1040, then we go back to main memory, and grab a new block of data. This new block of data overwrites the current block of data in the cache. This is called a <strong>miss</strong></p>
<p>This is why good special locality is important. Since the cache stores a chunk of memory, if the memory we are accessing together is close together, then the cache is much more likely to have a hit than a miss.</p>
<p><strong>Memory Organization</strong></p>
<p>In a cache, those blocks of data are called <strong>lines</strong>.
A cache has 3 parameters, S, E, and B. A cache will always have S=2^s <strong>sets</strong>, and each set will have E=2^e lines. </p>
<p>A line contains the following a<strong>valid bit</strong> referred to as v which indicates if the data stored in the line is valid or not. As well as B=2^b bytes that store the actual cached data copied from memory.</p>
<p>Total capacity of a cache = S*E*B bytes.
4.8(bottom next msg) shows an image of a cache.</p>
<p>Given an address, to check if the data is in the cache, the address split into three smaller parts. t bits for the tag, s bits for the set index, and b bits for the block offset.</p>
<p>First t bits are the tag. Next s bits are the set index. Last b bits for block offset.</p>
<img src="./images3824/5.png" alt="img id 5, if you see this msg email me!">
<p>From there we perform the following steps</p>
<ol>
<li>Use the set index in the address to locate a set.</li>
<li>Compare the tags in all lines of the set with the one in the address.
If a tag is a match, that is a hit, the data is in that line starting from the block offset.
If there isn&#39;t a match, it&#39;s a miss, so we go and copy the data from memory instead.</li>
</ol>
<p><strong>Direct-Mapped Cache</strong></p>
<ul>
<li>Simplest form of cache</li>
<li>Each set has only 1 line</li>
<li>Example on making a direct mapped cache below</li>
<li>Assumptions we can make for the example<ul>
<li>Main Memory uses 4 bit addresses, so total of 16 bytes of data, each byte is uniquely addressed.</li>
<li>Mini cache is direct mapped so 1 line per set</li>
<li>Cache has a total of 4 sets, so s=2</li>
<li>Each set can store 2 bytes of data so b=1</li>
<li>Each time we retrieve 1 byte of data
-The cache starts empty</li>
</ul></ul>
<p>Pictures are attached at bottom of message.</p>
<p>Because each address has 4 bits, and s=2 and b=1, t=4-2-1. So t=1</p>
<p>Now lets run the requests one at a time in this order: <code>0b0000</code>, <code>0b0001</code>, <code>0b0111</code>, <code>0b1000</code>, and <code>0b0000</code></p>
<p>Since we know the first t bits are for the tag, the next s bits are for set index, and the last b bits are for the block offset, and all bits are 0 for this address, all of those values will be 0.
Since the cache is empty, Set 00 has nothing, so it is empty, so it is a miss.
So we go to memory and grab <code>M[0b0000]</code> and <code>M[0b0001]</code>, we grab both because in the assumptions we know each set can store 2 bytes, so we grab 2 bytes of data.</p>
<p>After that we load 1 byte starting at offset 0 from that line, so <code>M[0b0000]</code> goes back to the register files.</p>
<p>Next we load <code>0b0001</code>, and it&#39;s 2 bits after the tag are 0b00, so we index to set 00. We notice the valid bit is turned on, and the tag also matches what is in the data. So we have a hit. Then we index by 1 since the last bit in this example is the block offset and we send the data which is <code>M[0001]</code> to the register file.</p>
<img src="./images3824/6.png" alt="img id 6, if you see this msg email me!">
<img src="./images3824/7.png" alt="img id 7, if you see this msg email me!">
<img src="./images3824/8.png" alt="img id 8, if you see this msg email me!">
<p>Now for <code>0b0111</code>, the set index becomes 0b11, so we have a miss. So we go to main memory and copy both <code>M[0110]</code> and <code>M[0111]</code> to the cache. Then we set the valid bit to 1, set the tag to 0, and since the block offset is 1, we copy the <code>M[0111]</code> to the register file.</p>
<p>The reason why we copied <code>M[0110]</code> and not <code>M[1000]</code> is because the last bit is the block offset and <code>M[0111]</code> is occupying the block offset at 1, so we copy a value with the same tag and set index but at a block offset of 0.</p>
<p>Next is <code>0b1000</code>. It has a set index of <code>0b00</code> so we go to set 00 and compare the tags. Since both of the tags of the values in set 00 are 0, and <code>0b1000</code> has a tag of 1, we have a miss. So we go to main memory, copy <code>M[0b1000</code> and <code>M[0b1001</code> to the cache.</p>
<p>Lastly is <code>0b0000</code> again. So we index to set 00, compare the tags and notice the tag is now 1, so we have a miss. So we must go to memory, copy <code>M[0b0000</code> and <code>M[0b0001</code> again and overwrite the values in set 00 and update the tag.</p>
<p>ngl im skipping real world example bc its the same stuff, if you don&#39;t understand direct mapped caches see 4.2.3.3 in the textbook for a real world example</p>
<img src="./images3824/9.png" alt="img id 9, if you see this msg email me!">
<img src="./images3824/10.png" alt="img id 10, if you see this msg email me!">
<p><strong>Associative Cache</strong></p>
<p>In a direct mapped cache, if ther is a conflict we must go to the memory and replace the dat ain the cache.</p>
<p>An associative cache has 2 lines per set, one with a tag of 0, and the other with a tag of 1.</p>
<p>C is the capacity of the cache, given a fixed value for C, we can adjust the three parameters to have different types of caches.</p>
<p>For example E=1 is a direct mapped cache, S=1 is a <strong>fully associative cache</strong>, and any other combinations is an <strong>E-way set associative cache</strong></p>
<p>The three different types of caches are shown in figure 4.12</p>
<p>For a direct mapped cache, since E=1, we&#39;ll have C=8=S*B capacity. Since B=2, and there are 4 sets.</p>
<p>For a 2 way set associative cache
Reduce the number of sets in half, so for this example S=2, but to keep B=2, there has to be 2 lines per set, so C=8 and S=E=B=2.</p>
<p>Now that there is only two sets, only 1 bit is needed for set index, so LSB is still block offset, and instead the two MSB are for the tag.</p>
<p>For a fully associative cache to keep capacity unchanged, 4 lines are needed. Since LSB is the block offset and we don&#39;t need a set index, the 3 MSBs of the address are used as the tag.</p>
<p>E-way set associative caches and 2-way caches have 2 possible problems.</p>
<p>If data is requested from the cache and empty lines are in the set, which line do we put the data in? A simple solution is to just put it in the next available line.</p>
<p>When there is a conflict, since each line has two bits per tag, there are 4 possible tags for each set total. Because there are only 2 lines per set, not every possible tag will be in a set. So which line do replace?</p>
<p>For the sake of this class, we will replace the earliest used line.</p>
<img src="./images3824/11.png" alt="img id 11, if you see this msg email me!">
<p>Above we discussed read transactions with a Cache</p>
<p>The issue with write transactions is due to multiple copies throughout the entire system. For example lets run <code>STR X0, [0x1000]</code>. This writes the data stored in <code>X0</code> at memory address <code>0x1000</code> if the data at this address has already been copied in the cache, do we only update the cache or the memory or both?</p>
<p>If we update only the cache, resulting in the cache and memory having two different values. If the cache gets replaced the new value will be gone and the memory still has the old value.</p>
<p>If we only update the memory, the delay is too long, and there will be data inconsistency. EX: Next is <code>LDR X0, [0x1000</code> would grab the data from cache, which is the old value instead of the new one in memory.</p>
<p>Write transactions also have a hit and miss. They are defined the same as read transactions (above).</p>
<p>There are 2 ways to deal with a write hit
<strong>Write-Through</strong>: write to both main memory and cache.
<strong>Write-Back</strong>: update the cache only first, and update main memory once the line is replaced.</p>
<p>There are 2 ways to deal with a write miss.
<strong>Write-Allocate</strong>: 
copy the line into the cache from main memory, then update the data in the cache
<strong>No-Write-Allocate</strong>: write straight to main memory without ignoring the cache.</p>
<p>Usually write through is paired with no-write-allocate, and write-back is paired with write-allocate.</p>
<p>For  write through + no-write-allocate, both cases only deal with main memory. For write-back +  write-allocate, both deal with cache first only and update main memory once the cache line is replaced.</p>
<p><strong>Multi-Level Caches</strong></p>
<p>L1, L2, L3, and so on are levels of cache. Lk holds part of the data copied from Lk+1, Lk is smaller and more expensive than Lk+1. Figure 4.13 shows an example of what an ARM chip looks like.</p>
<p>When buying a computer, terms like &quot;8-core&quot; or &quot;quadcore&quot; mean having multiple CPUs with the basic elements in this course. In recent architectures, L1 is actually two caches, d-cache for data, and i-cache for instructions.</p>
<p>If there are multiple cores in one processor package, it is common for there to be an L2 cache that connects all the cores. It is larger and slower than L1. Sometimes a computer can even have multiple processor packages, so each one is connected by one even larger and slower L3 cache. L3 is external in this case because it is not on a particular processor chip.</p>
<img src="./images3824/12.png" alt="img id 12, if you see this msg email me!">
<p><strong>Evaluation Metrics</strong></p>
<p>Miss rate = Times of data not found in cache/Total memory reference</p>
<p>An example of this is given a bitmap image of pixels, each pixel is represented with the struct below.</p>
<p>We will use a direct mapped cache of 128 bytes with 8 byte blocks.</p>
<pre><code class="lang-C"> typedefstruct{
   unsignedchar r;
   unsignedchar g;
   unsignedchar b;
   unsignedchar a;
 }
pixel_t;
pixel_t pixel[<span class="hljs-string">16</span>][<span class="hljs-symbol">16</span>];
</code></pre>
<p>Let&#39;s assume <code>sizeof(unsigned char) == 1</code>
pixel begins at memory address 0,
The cache starts empty
Variables i and j are stored in registers and accessing them won&#39;t cause a cache miss.</p>
<p>Now lets find the miss rate for the code below.</p>
<pre><code class="lang-C"> for(i=<span class="hljs-number">0</span><span class="hljs-comment">;i&lt;16;i ++){</span>
    for(<span class="hljs-keyword">j=0;j&lt;16;j++ </span>{
   pixel[i][<span class="hljs-keyword">j].r </span>=<span class="hljs-number">0</span><span class="hljs-comment">;</span>
   pixel[i][<span class="hljs-keyword">j].g </span>=<span class="hljs-number">0</span><span class="hljs-comment">;</span>
   pixel[i][<span class="hljs-keyword">j].b </span>=<span class="hljs-number">0</span><span class="hljs-comment">;</span>
   pixel[i][<span class="hljs-keyword">j].a </span>=<span class="hljs-number">0</span><span class="hljs-comment">;</span>
   }
 }
</code></pre>
<p>To do this we determine the memory and cache structure. Since C is a row-major-order language, the storage of matrix pixel_t will have rgba right next to eachother, repeating 1024 times, each one being represented by [i][j], with bytes that have consecutive [j] values being stored next to eachother.</p>
<p>Since we are using direct mapped cache, E=1, and each line has 8 = 2^3 bytes, so b=3. We also have a total capacity of 128=2^7 bytes, so we have 4 sets.</p>
<p>In the inner loop there are 4 write statements, so each iteration has 4 memory accesses. Each element in the memory also takes 4 bytes, while each line in the cache takes 8 bytes. So we can store two consecutive elements in each line. In this example it is [i][j] and [i][j+1]</p>
<p>In the inner loop, the first write in the first iteration will always be a miss. This is either due to there being no cache, or the line it is at being replaced. Then all eight bytes starting from pixel+i+j will be carried into the cache. This means the next 7 writes will all be hits.</p>
<p>It is possible to rewrite so it is identical in cache and memory access. It would become the image attached.</p>
<p>Knowing that 1/8 writes is a miss, we have a miss rate of 12.5%.</p>
<img src="./images3824/13.png" alt="img id 13, if you see this msg email me!">
<p><strong>Cache friendly code</strong></p>
<p>When we learned Big-O notation, we were told to ignore hardware differences.</p>
<p>We are tasked with writing code for matrix multiplication between two matrices a and b of the same size nXn. Three different implementations all with O(n^3) runtime are listed below.
Each element of the matrix takes 8 bytes.</p>
<pre><code class="lang-C"><span class="hljs-comment">/* ijk */</span>
 for (i = <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; n; i ++) {</span>
  for (<span class="hljs-keyword">j </span>= <span class="hljs-number">0</span><span class="hljs-comment">; j &lt; n; j ++) {</span>
    sum = <span class="hljs-number">0</span>.<span class="hljs-number">0</span><span class="hljs-comment">;</span>
    for (k = <span class="hljs-number">0</span><span class="hljs-comment">; k &lt; n; k ++){</span>
        sum+=a[i][k]*<span class="hljs-keyword">b[k][j];
</span>      }
    c[i][<span class="hljs-keyword">j] </span>= sum<span class="hljs-comment">;</span>
   }
 }
<span class="hljs-comment">/* kij*/</span>
for (k = <span class="hljs-number">0</span><span class="hljs-comment">; k &lt; n; k ++) {</span>
  for (i = <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; n; i ++) {</span>
     r = a[i][k]<span class="hljs-comment">;</span>
     for (<span class="hljs-keyword">j </span>= <span class="hljs-number">0</span><span class="hljs-comment">; j &lt; n; j ++){</span>
        c[i][<span class="hljs-keyword">j] </span>+= r * <span class="hljs-keyword">b[k][j];
</span>    }
  }
}
<span class="hljs-comment">/*jki*/</span>
for (<span class="hljs-keyword">j </span>= <span class="hljs-number">0</span><span class="hljs-comment">; j &lt; n; j ++) {</span>
  for (k = <span class="hljs-number">0</span><span class="hljs-comment">; k &lt; n; k ++) {</span>
    r = <span class="hljs-keyword">b[k][j];
</span>    for (i = <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; n; i ++){</span>
       c[i][<span class="hljs-keyword">j] </span>+= a[i][k] * r<span class="hljs-comment">;</span>
    }
  }
}
</code></pre>
<p>Now to analyze the miss rates of each one, assume the block size of the cache is 32 bytes, and can&#39;t store multiple rows. Also n is very large.</p>
<p>For method ijk, elements a and b are accesed each time, and for a[i][k] the row-number is fixed at i while k is constantly changing from 0 to n-1. Because the block size is 32, and each element takes 8 bytes. For every four elements, the first will hit and the other 3 will miss, leaving a miss rate of 25%. Since matrix-b gets each element column wise, the miss rate is 100%. The inner loop does not involve matrix c, so it is ignored.</p>
<p>For method kij, the inner loop does not involve matrix a, so it is ignored. Element b[k][j] iterates through all of the elements in each row, so the miss rate is also 0.25. Matrix c does the same so it&#39;s miss rate is also 0.25.</p>
<p>For jki, matrix b is not in the inner loop, so it is ignored. For matrices a and c, the inner loop iterates by the column, so both have a miss rate of 100%. </p>
<p>Figure 4.14 shows a graph of each method&#39;s cycles depending on the size of n.</p>
<img src="./images3824/14.png" alt="img id 14, if you see this msg email me!">
<h1 id="4-3-virtual-memory">4.3 Virtual Memory</h1>
<p>The addresses recieved by cache are called <strong>physical addresses</strong>, they are real addresses used in the real main memory.</p>
<p>Laptops currently are usually 16GB-8GB of physical RAM, which is shared among all programs running in the system. 16GB is not enough to run multiple applications. For example just Shudong&#39;s music player on macOS needs 392GB of memory space.</p>
<p>Locality also applies to computer programs as it does to computer data. For example if we are given assembly instructions they are usually stored close to eachother. Because of this branching does not send us too far in memory from where we branched from.</p>
<p><strong>A motivating example</strong></p>
<p>Lets assume our memory can only store 48 bytes and we want to run two programs simultaneously. The two programs have a total of 15 instructions. For the two programs there is a total of 60 bytes at minimum(15*4 bytes)</p>
<p>Locality teaches us that the executed instructions of a program are stored next to each other in memory, so the solution is to load parts of a program into memory, and when those parts have been executed, we swap them out and move to the next parts. </p>
<p>This is represented in 4.15--&gt;4.17 A real world example of this is we may have a lot of books, but can only bring 5 to campus. So we would only take the books we need to bring that day, and if on a different day we need a different book, we just swap a book in the bag. In that example books are the program and the bag is phyiscal memory.</p>
<p>The operating system provides <strong>virtual memory</strong> which is a memory management mechanism.</p>
<p>So far in this course all of our statements involving memory addresses besides ones which use a cache refer to virtual memory. If a virtual memory has n bits, each program will have the same set of N=2^n -1 unique virtual addresses, called a <strong>virtual address space</strong></p>
<p>Physical memory is byte addressed, if a physical memory has m bits, then then it has M=2^m -1 unique addresses is called the <strong>physical address space</strong>.</p>
<img src="./images3824/15.png" alt="img id 15, if you see this msg email me!">
<img src="./images3824/16.png" alt="img id 16, if you see this msg email me!">
<p>Despite the equations being the same, N &gt; M because virtual memory space simply has more bits.</p>
<p>It is possible to chop virtual memory space into <strong>virtual pages</strong> and only load some virtual pages into physical memory. Those virtual pages then become <strong>physical pages</strong></p>
<p>If a processor needs data or code in a page that isn&#39;t present, then it needs to find a page to swap out. If it can&#39;t, then a delay happens. This is basically lag from opening too many apps.</p>
<p>Physical pages are not dedicated to a specific process, so pages from two processes are all over the place and intertwined.</p>
<p>There is no order in physical memory, a prage in a low virtual memory address doesn&#39;t end up in a low physical memory address.</p>
<p>The pages are of equal size.</p>
<p>How does a processor know which physical address corresponds to which virtual memory address in which process? This uses concept switching which is not in this course, thats in OS. </p>
<p><strong>Address Translation</strong></p>
<p>The procedure of finding a physical memory address to a given virutal memory address is called <strong>address translation</strong></p>
<p>Assume the virtual memory space can be defined as V={0,1,...,N-1} and the physical memory space as P(0,1,...,M-1).</p>
<p>A function can be defined as MAP: V-&gt;P∪{ ∅}</p>
<p>a&#39; = MAP(a) where a&#39; is the translated physical address. If a&#39;  ∈ P, the virtual address can be successfully mapped to physical memory, and the data can be found there. If a&#39; = ∅, the virtual address can&#39;t be mapped to physical memory, this can happen if we are trying to use a variable that&#39;s currently in a virtual page but that page hasn&#39;t been loaded to physical memory yet. If the address can be mapped, it is a <strong>page hit</strong>, otherwise it is a <strong>page fault</strong>.</p>
<p>Translation occurs in a special hardware called the <strong>memory management unit</strong> (MMU), adding a MMU to the CPU looks like figure 4.19</p>
<p>MMU takes in a virtual memory address, and translates it to a physical address and sends it to the cache. It&#39;s managed and programmed by the operating system.</p>
<img src="./images3824/17.png" alt="img id 17, if you see this msg email me!">
<p>For the MMU to translate addresses, it just looks it up using the <strong>page table</strong> which is a 1-1 map stored in the main memory. It has two columns, the first being valid bit, and the other being <strong>Physical Page Number</strong> (PPN)</p>
<p>Each row is called a <strong>page table entry</strong>, the index to each page table entry is called the <strong>virtual page number </strong>(VPN). An example is given a virtual address we know the VPN of is 2, by looking it up in the table attached as an example, we know that the virtual page is located at address starting from <code>0xCC00</code> in physical memory. The valid bid indicates it&#39;s actually in physical memory and if it is 1 we have a page hit, and if it is 0 we have a page fault.</p>
<p>When there is a page fault, the main memory will pick a page called the victim page and evict it to secondary storage such as a hard drive, then the page we need will be taken back to main memory from the hard drive and the address will be translated by the MMU.</p>
<p>A virtual address of n bits can be split into 2 parts.
Bits n-1 -&gt; p are the VPN, while bits p-1 -&gt; 0 are the virtual page offset (VPO)</p>
<p>The VPN is used to index to a row in the page table, and if the valid bit at that row is a 1, then it&#39;s a hit. From there we index by the VPO to get the physical page of that row. The VPO then becomes the physical page offset (PPO) without change, and PPN is pulled from the table. The two are combined to make the physical address.</p>
<p>If PPO = 0, we want the first byte of that page. If PPO = 1600, we want the 1601th byte of that page. p bits are needed to represent the offset, and each byte has it&#39;s own address so there will be 2^p bytes stored. Therefore the <strong>page size</strong> P=2^p</p>
<p>To prevent the MMU from being overwritten in the cache because it is constantly needed, a TLB or <strong>translation lookaside buffer</strong> can be installed on the MMU as seen in figure 4.20. The TLB is a set-associative cache where each line stores one page table entry.</p>
<img src="./images3824/18.png" alt="img id 18, if you see this msg email me!">
<img src="./images3824/19.png" alt="img id 19, if you see this msg email me!">
<p>The VPN is then split into the TLB tag (TLBT) and the TLB index (TLBI)</p>
<p>These two parts index a page table entry (line) in the TLB.</p>
<p><strong>From Virtual Address to Data</strong></p>
<p>Lets say we wrote <code>char c = *ptr; //ptr is a char pointer</code></p>
<p>Upon compiling this line becomes <code>LDRB W0, [X1]</code> in assembly.</p>
<p>In that example <code>X1</code> is the virtual memory address, assume <code>X1=0x1DDE</code> and the following.
Memory is byte addressable
Virtual Addresses are 16 bits wide
Physical Addresses are 13 bits wide.
Page size is 512 bytes.
TLB is an 8-way set associative with 16 entries.
The cache is a 2-way set associative with a 4 byte line size and 16 total lines.
First 32 pages of TLB are shown below in table 4.3</p>
<p>First we decide bit representations of virtual addresses, this is done by figuring out which part is VPN and which part is VPO, this can be found using the page table and TLB.</p>
<p>We know 512 = 2^9 bytes, so 9 bits are needed to represent page offsets. So bits [8-0] are VPO and the remaining 7 are VPN.</p>
<p>VPN has TLBI and TLBT, and since TLB has only 2 sets, only 1 bit is needed to index the TLB. So bit 9 becomes TLBI, and the first 6 bits become TLBT.</p>
<p>Next we write out and parse the virtual address in binary</p>
<p>Now that the components of the virtual address have been identified, we can turn the address given at the start into binary and label the parts using the bit subsets we found in the previous step.</p>
<p>Next we index into TLB and in this example we index into set 0 to see if any matches our TLBT of <code>0x07</code> in this example. None do, so we have a TLB miss, so VPN is used to check the page table. MMU will go to main memory and fetch the page table entry given VPN.</p>
<p>Looking at table 4.3 we can see that the page table entry for <code>0x0E</code> has a PPN of <code>0x1</code> And it&#39;s valid bit is 1, so we have a page hit. MMU takes this page back and translates it to physical address.</p>
<img src="./images3824/20.png" alt="img id 20, if you see this msg email me!">
<p>Next is translate to physical address. We have the PPN, so we put it in the MMU and attach it with VPO to form the physical address. The result is <code>0x11DE</code>. This is where ptr points to in physical memory.</p>
<p>Next we write out and parse the physical address in binary and identify the cache components. In this example cache has 8 sets, so we need 3 bits for set index, and each line has 4 bytes, so 2 bytes for block offset. 13-2-3 = 8, so 8 bits for tag.</p>
<p>Visit the cache using the parameters generated in the previous step, and determine if we have a cache hit or miss. If we have a hit (in this example we do), the byte is brought back to the CPU and stored in <code>W0</code>,</p>
<p>If we had a miss, we&#39;d have to go to memory, evict the line with tag <code>0x1E</code> in set 7, and write the line in cache back to memory assuming we are using write-allocate protocol, and copy the line we want to cache.</p>
</main>
  </body>
</html>